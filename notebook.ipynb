{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import here\n",
    "import PIL\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "training_file = \"data/train.p\"\n",
    "validation_file = \"data/valid.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "# Number of validation examples.\n",
    "n_valid = len(X_valid)\n",
    "# Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# The shape of an traffic sign image\n",
    "image_shape = X_train[0].shape[:-1]\n",
    "\n",
    "# Number of unique classes/labels in the dataset.\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_valid)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization.\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(n_classes), np.bincount(y_train), 0.5, color='r')\n",
    "ax.set_xlabel('Signs')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('The count of each sign')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for c in range(n_classes):\n",
    "    i = random.choice(np.where(y_train == c)[0])\n",
    "    plt.subplot(8, 8, c+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('class: {}'.format(c))\n",
    "    plt.imshow(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Design and Test a Baseline Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with open(file_path, mode='rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.features = data['features']\n",
    "            self.labels = data['labels']\n",
    "            self.count = len(self.labels)\n",
    "            self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        if self.transform is not None:\n",
    "            feature = self.transform(feature)\n",
    "        return (feature, self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, gray=False):\n",
    "        super(BaselineNet, self).__init__()\n",
    "        input_chan = 1 if gray else 3\n",
    "        self.conv1 = nn.Conv2d(input_chan, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PickledDataset(training_file, transform=transforms.ToTensor())\n",
    "valid_dataset = PickledDataset(validation_file, transform=transforms.ToTensor())\n",
    "test_dataset = PickledDataset(testing_file, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return x.to(device), y.to(device, dtype=torch.int64)\n",
    "\n",
    "train_loader = WrappedDataLoader(train_loader, preprocess)\n",
    "valid_loader = WrappedDataLoader(valid_loader, preprocess)\n",
    "test_loader = WrappedDataLoader(test_loader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, x, y, opt=None):\n",
    "    loss = loss_func(model(x), y)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_batch(model, loss_func, x, y):\n",
    "    output = model(x)\n",
    "    loss = loss_func(output, y)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    correct = pred == y.view(*pred.shape)\n",
    "    \n",
    "    return loss.item(), torch.sum(correct).item(), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Train model\n",
    "        model.train()\n",
    "        losses, nums = zip(*[loss_batch(model, loss_func, x, y, opt) for x, y in train_dl])\n",
    "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        # Validation model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in valid_dl])\n",
    "            valid_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            valid_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "                  f\"Train loss: {train_loss:.6f}\\t\"\n",
    "                  f\"Validation loss: {valid_loss:.6f}\\t\",\n",
    "                  f\"Validation accruacy: {valid_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_func, dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in dl])\n",
    "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        test_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "        \n",
    "    print(f\"Test loss: {test_loss:.6f}\\t\"\n",
    "          f\"Test accruacy: {test_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment and find tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel separation\n",
    "red_chan = np.reshape(X_train[:,:,:,0], -1)/255\n",
    "green_chan = np.reshape(X_train[:,:,:,1], -1)/255\n",
    "blue_chan = np.reshape(X_train[:,:,:,2], -1)/255\n",
    "\n",
    "# mean\n",
    "means = [np.mean(red_chan), np.mean(green_chan), np.mean(blue_chan)]\n",
    "\n",
    "# std\n",
    "stds = [np.std(red_chan), np.std(green_chan), np.std(blue_chan)]\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalize\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3340, 0.3117, 0.3209), (0.2717, 0.2599, 0.2658))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), preprocess)\n",
    "valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), preprocess)\n",
    "test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast-limited adaptive histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHE:\n",
    "    def __init__(self, clipLimit=2.0, tileGridSize=(8, 8)):\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self, im):\n",
    "        img_yuv = cv2.cvtColor(im, cv2.COLOR_RGB2YUV)\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "        img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n",
    "        img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "        return img_output\n",
    "    \n",
    "class CLAHE_RGB:\n",
    "    def __init__(self, clipLimit=2.0, tileGridSize=(8, 8)):\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "        \n",
    "    def __call__(self, im):\n",
    "        r,g,b = cv2.split(im)\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "        r,g,b = clahe.apply(r), clahe.apply(g), clahe.apply(b)\n",
    "        return cv2.merge([r,g,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = CLAHE()\n",
    "plt.figure(figsize=(16, 16))\n",
    "for c in range(n_classes):\n",
    "    i = random.choice(np.where(y_train == c)[0])\n",
    "    plt.subplot(8, 8, c+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('class: {}'.format(c))\n",
    "    plt.imshow(clahe(X_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLAHE\n",
    "X_train_clahe = np.array([clahe(x) for x in X_train])\n",
    "\n",
    "# Channel separation\n",
    "red_chan = np.reshape(X_train_clahe[:,:,:,0], -1)/255\n",
    "green_chan = np.reshape(X_train_clahe[:,:,:,1], -1)/255\n",
    "blue_chan = np.reshape(X_train_clahe[:,:,:,2], -1)/255\n",
    "\n",
    "# mean\n",
    "means = [np.mean(red_chan), np.mean(green_chan), np.mean(blue_chan)]\n",
    "\n",
    "# std\n",
    "stds = [np.std(red_chan), np.std(green_chan), np.std(blue_chan)]\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    CLAHE(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4898, 0.4619, 0.4708), (0.2476, 0.2441, 0.2514))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), preprocess)\n",
    "valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), preprocess)\n",
    "test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = CLAHE_RGB()\n",
    "plt.figure(figsize=(16, 16))\n",
    "for c in range(n_classes):\n",
    "    i = random.choice(np.where(y_train == c)[0])\n",
    "    plt.subplot(8, 8, c+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('class: {}'.format(c))\n",
    "    plt.imshow(clahe(X_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLAHE\n",
    "X_train_clahe = np.array([clahe(x) for x in X_train])\n",
    "\n",
    "# Channel separation\n",
    "red_chan = np.reshape(X_train_clahe[:,:,:,0], -1)/255\n",
    "green_chan = np.reshape(X_train_clahe[:,:,:,1], -1)/255\n",
    "blue_chan = np.reshape(X_train_clahe[:,:,:,2], -1)/255\n",
    "\n",
    "# mean\n",
    "means = [np.mean(red_chan), np.mean(green_chan), np.mean(blue_chan)]\n",
    "\n",
    "# std\n",
    "stds = [np.std(red_chan), np.std(green_chan), np.std(blue_chan)]\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    CLAHE_RGB(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4821, 0.4713, 0.4801), (0.2411, 0.2443, 0.2438))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), preprocess)\n",
    "valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), preprocess)\n",
    "test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHE_GRAY:\n",
    "    def __init__(self, clipLimit=2.0, tileGridSize=(8, 8)):\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self, im):\n",
    "        img_y = cv2.cvtColor(im, cv2.COLOR_RGB2YUV)[:,:,0]\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "        img_y = clahe.apply(img_y)\n",
    "        img_output = img_y.reshape(img_y.shape + (1,))\n",
    "        return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = CLAHE_GRAY()\n",
    "plt.figure(figsize=(16, 16))\n",
    "for c in range(n_classes):\n",
    "    i = random.choice(np.where(y_train == c)[0])\n",
    "    plt.subplot(8, 8, c+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('class: {}'.format(c))\n",
    "    plt.imshow(clahe(X_train[i]).squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLAHE\n",
    "X_train_clahe = np.array([clahe(x) for x in X_train])\n",
    "\n",
    "gray_chan = np.reshape(X_train_clahe[:,:,:,0], -1)/255\n",
    "print(np.mean(gray_chan))\n",
    "print(np.std(gray_chan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    CLAHE_GRAY(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4715,),(0.2415,))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), preprocess)\n",
    "valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), preprocess)\n",
    "test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet(gray=True).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use weighted sampler\n",
    "class_sample_count = np.bincount(y_train)\n",
    "weights = 1 / np.array([class_sample_count[y] for y in y_train])\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 43 * 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_data_transforms = transforms.Compose([\n",
    "    CLAHE_GRAY(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BICUBIC),\n",
    "        transforms.RandomAffine(0, translate=(0.2, 0.2), resample=PIL.Image.BICUBIC),\n",
    "        transforms.RandomAffine(0, shear=20, resample=PIL.Image.BICUBIC),\n",
    "        transforms.RandomAffine(0, scale=(0.8, 1.2), resample=PIL.Image.BICUBIC)\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4715,),(0.2415,))\n",
    "])\n",
    "test_data_transforms = transforms.Compose([\n",
    "    CLAHE_GRAY(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4715,),(0.2415,))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=train_data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=test_data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=test_data_transforms)\n",
    "\n",
    "train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, sampler=sampler), preprocess)\n",
    "valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), preprocess)\n",
    "test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balacned_y_train = np.array([], dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, y in train_loader:\n",
    "        y = y.to(device, dtype=torch.int64)\n",
    "        balacned_y_train = np.append(balacned_y_train, y.cpu().numpy())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(n_classes), np.bincount(balacned_y_train), 0.5, color='r')\n",
    "ax.set_xlabel('Signs')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('The count of each sign')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_np(img):\n",
    "    img = img.numpy().transpose((1, 2, 0)).squeeze()\n",
    "    mean = 0.4715\n",
    "    std = 0.2415\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(train_loader))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(y)):\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title('class: {}'.format(y[i]))\n",
    "        plt.imshow(convert_image_np(x[i]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet(gray=True).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization, Dropout, Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 100, 5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(100, 150, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, 2, padding=1)\n",
    "        self.fc1 = nn.Linear(250 * 4 * 4, 300)\n",
    "        self.fc1_bn = nn.BatchNorm1d(300)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(300, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.elu(self.conv1(x)))\n",
    "        x = self.pool(F.elu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool(F.elu(self.conv3(x)))\n",
    "        x = x.view(-1, 250 * 4 * 4)\n",
    "        x = F.elu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        # Train model\n",
    "        model.train()\n",
    "        losses, nums = zip(*[loss_batch(model, loss_func, x, y, opt) for x, y in train_dl])\n",
    "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        # Validation model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in valid_dl])\n",
    "            valid_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            valid_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "                  f\"Train loss: {train_loss:.6f}\\t\"\n",
    "                  f\"Validation loss: {valid_loss:.6f}\\t\",\n",
    "                  f\"Validation accruacy: {valid_accuracy:.3f}%\")\n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print(f\"Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...\")\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "                valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrafficSignNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = torch.load('model.pt', map_location=device)\n",
    "model.load_state_dict(check_point)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial transformer networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StnNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 100, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(150)\n",
    "        self.conv3 = nn.Conv2d(150, 250, kernel_size=2, padding=1)\n",
    "        self.fc1 = nn.Linear(250 * 4 * 4, 300)\n",
    "        self.fc1_bn = nn.BatchNorm1d(300)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(300, 43)\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.loc_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 4 * 4, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.loc_net(x)\n",
    "        xs = xs.view(-1, 10 * 4 * 4)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        x = self.pool(F.elu(self.conv1(x)))\n",
    "        x = self.pool(F.elu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool(F.elu(self.conv3(x)))\n",
    "        x = x.view(-1, 250 * 4 * 4)\n",
    "        x = F.elu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StnNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the STN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        data = next(iter(test_loader))[0].to(device)\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_tensor = model.stn(data).cpu()\n",
    "\n",
    "        input_grid = convert_image_np(make_grid(input_tensor))\n",
    "        transformed_grid = convert_image_np(make_grid(transformed_tensor))\n",
    "\n",
    "        # Plot the results side-by-side\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        fig.set_size_inches((16, 16))\n",
    "        ax[0].imshow(input_grid)\n",
    "        ax[0].set_title('Dataset Images')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(transformed_grid)\n",
    "        ax[1].set_title('Transformed Images')\n",
    "        ax[1].axis('off')\n",
    "        \n",
    "visualize_stn()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
