{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import here\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "training_file = \"data/train.p\"\n",
    "validation_file = \"data/valid.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "# Number of validation examples.\n",
    "n_valid = len(X_valid)\n",
    "# Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# The shape of an traffic sign image\n",
    "image_shape = X_train[0].shape[:-1]\n",
    "\n",
    "# Number of unique classes/labels in the dataset.\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_valid)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization.\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(n_classes), np.bincount(y_train), 0.5, color='r')\n",
    "ax.set_xlabel('Signs')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('The count of each sign')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for c in range(n_classes):\n",
    "    i = random.choice(np.where(y_train == c)[0])\n",
    "    plt.subplot(8, 8, c+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('class: {}'.format(c))\n",
    "    plt.imshow(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Design and Test a Baseline Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with open(file_path, mode='rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.features = data['features']\n",
    "            self.labels = data['labels']\n",
    "            self.count = len(self.labels)\n",
    "            self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        if self.transform is not None:\n",
    "            feature = self.transform(feature)\n",
    "        return (feature, self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PickledDataset(training_file, transform=transforms.ToTensor())\n",
    "valid_dataset = PickledDataset(validation_file, transform=transforms.ToTensor())\n",
    "test_dataset = PickledDataset(testing_file, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return x.to(device), y.to(device, dtype=torch.int64)\n",
    "\n",
    "train_loader = WrappedDataLoader(train_loader, preprocess)\n",
    "valid_loader = WrappedDataLoader(valid_loader, preprocess)\n",
    "test_loader = WrappedDataLoader(test_loader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, x, y, opt=None):\n",
    "    loss = loss_func(model(x), y)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_batch(model, loss_func, x, y):\n",
    "    output = model(x)\n",
    "    loss = loss_func(output, y)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    correct = pred == y.view(*pred.shape)\n",
    "    \n",
    "    return loss.item(), torch.sum(correct).item(), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    model.zero_grad()\n",
    "    for epoch in range(epochs):\n",
    "        # Train model\n",
    "        model.train()\n",
    "        losses, nums = zip(*[loss_batch(model, loss_func, x, y, opt) for x, y in train_dl])\n",
    "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        # Validation model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in valid_dl])\n",
    "            valid_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            valid_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "                  f\"Train loss: {train_loss:.6f}\\t\"\n",
    "                  f\"Validation loss: {valid_loss:.6f}\\t\",\n",
    "                  f\"Validation accruacy: {valid_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_func, dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, corrects, nums = zip(*[valid_batch(model, loss_func, x, y) for x, y in dl])\n",
    "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        test_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "        \n",
    "    print(f\"Test loss: {test_loss:.6f}\\t\"\n",
    "          f\"Test accruacy: {test_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and find tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel separation\n",
    "red_chan = np.reshape(X_train[:,:,:,0], -1)/255\n",
    "green_chan = np.reshape(X_train[:,:,:,1], -1)/255\n",
    "blue_chan = np.reshape(X_train[:,:,:,2], -1)/255\n",
    "\n",
    "# mean\n",
    "means = [np.mean(red_chan), np.mean(green_chan), np.mean(blue_chan)]\n",
    "\n",
    "# std\n",
    "stds = [np.std(red_chan), np.std(green_chan), np.std(blue_chan)]\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalize\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3340, 0.3117, 0.3209), (0.2717, 0.2599, 0.2658))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=data_transforms)\n",
    "valid_dataset = PickledDataset(validation_file, transform=data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use weighted sampler\n",
    "class_sample_count = np.bincount(y_train)\n",
    "weights = 1 / np.array([class_sample_count[y] for y in y_train])\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 43 * 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "# Data augmentation\n",
    "train_data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "#     transforms.RandomAffine(10, translate=(0.2,0.2), scale=(0.8,1.2), shear=10, resample=PIL.Image.BILINEAR),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomAffine(0, translate=(0.2, 0.2)),\n",
    "        transforms.RandomAffine(0, shear=20),\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3398, 0.3117, 0.3210), (0.2755, 0.2647, 0.2712))\n",
    "])\n",
    "test_data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3398, 0.3117, 0.3210), (0.2755, 0.2647, 0.2712))\n",
    "])\n",
    "\n",
    "train_dataset = PickledDataset(training_file, transform=train_data_transforms)\n",
    "test_dataset = PickledDataset(testing_file, transform=test_data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balacned_y_train = np.array([], dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, y in train_loader:\n",
    "        y = y.to(device, dtype=torch.int64)\n",
    "        balacned_y_train = np.append(balacned_y_train, y.cpu().numpy())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(n_classes), np.bincount(balacned_y_train), 0.5, color='r')\n",
    "ax.set_xlabel('Signs')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('The count of each sign')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x, y = next(iter(train_loader))\n",
    "    x, y = x.to(device).cpu().numpy(), y.to(device).cpu().numpy()\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(y)):\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title('class: {}'.format(y[i]))\n",
    "        image = np.transpose(x[i], (1, 2, 0))\n",
    "        mean = np.array([0.3398, 0.3117, 0.3210])\n",
    "        std = np.array([0.2755, 0.2647, 0.2712])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        plt.imshow(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss = train()\n",
    "    valid_loss, accuracy = validation()\n",
    "    print('[Epoch #{}] Training Loss: {:.6f}\\tValidation Loss: {:.6f}\\tValidation Accuracy: {:.3f}%'.format(epoch, train_loss, valid_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast-limited adaptive histogram equalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained model VS Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial transformer networks"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
